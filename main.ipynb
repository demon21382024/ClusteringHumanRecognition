{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7fee26",
   "metadata": {},
   "source": [
    "# Project 03: Clustering Human Recognition\n",
    "\n",
    "Este proyecto busca detectar qué acción está realizando un humano por medio de un datasets de videos donde se realizan interacciones como: *tocar instrumentos, interacción de humanos con objetos, humanos saludándose, humanos abrazándose, etc*. Para cada acción se cuentan con como mínimo 400 video clips."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772e71b",
   "metadata": {},
   "source": [
    "---\n",
    "### 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be7ec919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2359c",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Video features extraction\n",
    "\n",
    "Cada clip tiene asociado con una sola acción humana y dura aproximadamente 10 sgundos. Este proyecto usa un subset del dataset original, se utilizará la librería de Video Fetures y Gluon para extraer las características del video.\n",
    "\n",
    "Para proceder con el almacenamiento de los datos se utilizará un almacenamiento del tipo **HDF5** por medio de la librería `h5py`. De igual forma se procede a utiizar un modelo etiquetado como: **Action Recognition** para la extracción de características, esto es así pues a comparación del otro modelo **Frame-wise** nosotros tenemos como data acciones dinámicas y no estáticas.\n",
    "\n",
    "El costo computacional para reconocimiento de acciones será más costoso pues analizará más frames por segundo, por ese motivo se elijió el modelo **S3D (Kinetics 400)**, que si bien no es el más potente o el que brinda más características, pero si aquel que tiene un buen **tradeoff** entre eficiencia y precisión.\n",
    "\n",
    "Necesitamos **extraer las características** de nuestros videos por medio de la librería que se ejecuta a nivel terminal, con el siguiente comando podremos transformar un listado de videos tal que obtenemos características para cada uno. Es así que contamos en la [documentación](https://v-iashin.github.io/video_features/models/s3d/) con argumentos opcionales, un ejemplo es el siguiente `python main.py feature_type = s3d video_paths=\"[./qyjw13RCjZk_000017_000027.mp4]\" show_pred = true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77886805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de videos cargados: 20\n",
      "Shape de nuestra matriz de videos - features: (20, 1024)\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv('./data/train.csv', header = None, names = ['video_id', 'activity'])\n",
    "\n",
    "video_features_dir = './data/s3d'\n",
    "video_features = []\n",
    "video_labels = []\n",
    "\n",
    "for each_video in glob.glob(os.path.join(video_features_dir, '*.npy')):\n",
    "    video_id = os.path.basename(each_video).split('_')[0]\n",
    "    \n",
    "    current_video_feature = np.load(each_video)\n",
    "    current_video_feature_avg = np.mean(current_video_feature, axis = 0)\n",
    "    video_features.append(current_video_feature_avg)\n",
    "\n",
    "    current_video_label = labels_df[labels_df['video_id'] == video_id]['activity'].values[0]\n",
    "    video_labels.append(current_video_label)\n",
    "\n",
    "video_features_stacked = np.vstack(video_features)\n",
    "\n",
    "print(f'Número de videos cargados: {len(video_labels)}')\n",
    "print(f'Shape de nuestra matriz de videos - features: {video_features_stacked.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e3f99a",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Data analysis \n",
    "\n",
    "Para iniciar nuestra exploración, primero aplicaremos una **reducción de dimensionalidad**. Esto se debe a que actualmente se cuenta con un vector que representa cada video, tal que este vector tiene $1024$ características (dimensiones).\n",
    "\n",
    "Usaremos **PCA (Principal Component Analysis)** que nos permitirá reducir la dimensionalidad manteniendo la mayor parte de la varianza de los datos, a mayor cantidad de varianza habrá mayor entropía por lo que por ende seguiremos contando con gran cantidad de **información representativa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66c11895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components = 20)\n",
    "video_features_pca_reduced = pca.fit_transform(video_features_stacked)\n",
    "\n",
    "video_features_pca_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291ffef",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Dimensionality reduction\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee1f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2673e544",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f779d33",
   "metadata": {},
   "source": [
    "#### 3.1 Partitioning Method: Median Shift\n",
    "---\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7e662",
   "metadata": {},
   "source": [
    "#### 3.1 Density-based Method: DBSCAN\n",
    "---\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e555a",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Classification metrics\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13d9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
